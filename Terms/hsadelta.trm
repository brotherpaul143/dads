# *created  "Thu Jul  6 15:58:18 2006" *by "Paul E. Black"
# *modified "Thu Aug 14 12:14:34 2008" *by "Paul E. Black"

# $Log: hsadelta.trm,v $
# Revision 1.3  2008/08/14 16:14:36  black
# Update HTML line break (br) tags to be XML/XHMTL compatible
#
# Revision 1.2  2007/05/14 14:13:02  black
# Update Agarwal URL in BIB
#
# Revision 1.1  2006/07/07 14:47:21  black
# Initial revision
#

# entry name
@NAME=hsadelta
# _A_lgorithm, algo _T_echnique, _D_efinition, _P_roblem, or data _S_tructure
@TYPE=A
# autom basic theory search sort tree graph combin numeric etc. see areas.data
@AREA=crypt
# the definition
@DEFN=An {algorithm} to produce a sequence of insert and copy commands
(an {edit script}) which creates a new version file from a reference
file.
To begin, {hash#hash function} every block of the reference file and
store every hash in a hash value {array}.  Build a {suffix array} and
three other data 
structures for quick access.  Beginning at the first location in the
version file, hash a block and look for the longest match in the
reference file.  Upon a match, encode an insert back to the previous
match and a copy of the match.  If no match, look at the next
location.  At the end, encode an insert for remaining unmatched
characters.
# formal definition or {cross reference} to an entry
@FORML=
# comma-sep list of pure names that this is Also Known As.
@AKA=
# other cross-listings solely for the web, such as name or spelling variants
@WEB=

#    These are all comma-separated lists of {cross references}
# Generalization: "I am a kind of ..."
@IMA=
# Specialization: "... is a kind of me."
@VARIANT=
# Aggregate parent: "I am a part of or used in ..."
@IMIN={longest common subsequence}
# Aggregate child: "... is a part of or used in me."
@INME={Bloom filter}, {hash table}, {binary search}
# Other cross references that don't fit the above.  printed as "See also ..."
@XREFS=

# bib refs, eg, to defining article (pure HTML).  printed within <p>..</p>
@BIB=
<strong>Ramesh C. Agarwal</strong>, <strong>Karan Gupta</strong>,
<strong>Shaili Jain</strong>, and <strong>Suchitra Amalapurapu</strong>,
<em>An approximation to the greedy algorithm for differential
compression</em>, IBM Journal of Research &amp; Development,
50(1):149-166, January 2006.
<br />
Available from <a
href="http://researchweb.watson.ibm.com/journal/rd/501/agarwal.html">http://www.research.ibm.com/journal/rd/501/agarwal.html</a>
DOI: 10.1147/rd.501.0149.

# any notes.  these are not printed in the final dictionary
@NOTES=There are many differential compression algorithms, such as
vcdiff, xdelta, and zdelta.  See Agarwal et. al.  I happened to find
this one, and it uses neat data structures.
</p>

<p>
The three other data structures for quick access to the suffix array
are: 
the quick index array, a {Bloom filter} using one {hash function},
the block hash table, and the pointer array, an array of pointers into
the block hash table.  The quick index array allows most (97%) version
file block hashes to be rejected as having no match.  The block hash
table has pointers into the suffix array.  The pointer array can be
viewed as a table of the {hash function} into the block hash table,
which uses {open addressing} in case of {collisions}.
</p>

<p>
When a match is found, the suffix array is searched, via {binary
search} in the block hash table, to find the longest matching sequence
of blocks.  Then the match is extended character by character as far
as possible.

# any historical notes
@HISTORY=

# implementation(s) (pure HTML)
@IMPL=
# further explanation (pure HTML)
@LINKS=
# author's initials (see authors.data)
@AUTHOR=PEB
# end $Source: /home/black/Workspace/dads/Terms/RCS/hsadelta.trm,v $
