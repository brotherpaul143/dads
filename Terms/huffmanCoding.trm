# *created  "Wed Feb  3 13:42:31 1999" *by "Paul E. Black"
# *modified "Mon Mar  7 09:56:01 2005" *by "Paul E. Black"

# $Log: huffmanCoding.trm,v $
# Revision 1.9  2005/03/07 15:07:06  black
# Update Vinokur's MathForum URL.
#
# Revision 1.8  2005/02/02 20:25:40  black
# IM not A greedy algorithm; it is an algo. technique used INME.
#
# Revision 1.7  2005/01/10 16:49:56  black
# Fix typo in Vinokur's URL in NOTES.
#
# Revision 1.6  2005/01/04 15:28:17  black
# Remove Vermeer's URL - gone.  Fix typo.  Show John Morris' name.
#
# Revision 1.5  2004/12/23 21:34:36  black
# Remove bad trailing space in VARIANT.
#
# Revision 1.4  2004/12/17 17:24:01  black
# Make more XHTML compliant.
#
# Revision 1.3  2004/11/01 16:40:14  black
# Change encoding to coding.  Improve DEF'N.  Add original BIB and links
# to Wikipedia.
#
# Revision 1.2  2004/10/29 13:23:35  black
# Refine XREFS into IMA, etc.  Add RCS keywords.
#

# entry name
@NAME=Huffman coding
# _A_lgorithm, _D_efinition, _P_roblem, or data _S_tructure
@TYPE=A
# basic, numeric, search, sort, graph, combin(atorial), tree, theory
@AREA=crypt
# the definition
@DEFN=A minimal variable-length character coding based on the frequency of 
each character.  First, each character becomes a trivial 
{binary tree}, with the
character as the only {node}.  The character's frequency is the tree's
frequency.  Two trees with the least frequencies are joined as the
subtrees of a
new root that is assigned the sum of their frequencies.  This is
repeated until all characters are in one tree.  One code bit represents
each level.  Thus more frequent characters are near the {root} and
are coded with few bits, and rare characters are far from the root
and are coded with many bits.
# formal definition or {cross reference} to an entry
@FORML=
# comma-sep list of pure names that this is Also Known As.
@AKA=static Huffman coding
# other cross-listings solely for the web, such as word or spelling variants
@WEB=Huffman encoding, Hufman encoding, Huffmann encoding

#    These are all comma-separated lists of {cross references}
# Generalization: "I am a kind of ..."
@IMA=
# Specialization: "... is a kind of me."
@VARIANT={adaptive Huffman coding}, {k-ary Huffman coding}
# Aggregate parent: "I am a part of or used in ..."
@IMIN=
# Aggregate child: "... is a part of or used in me."
@INME={full binary tree}, {priority queue}, {greedy algorithm}
# Other cross references that don't fit the above.  printed as "See also ..."
@XREFS={arithmetic coding}, 
{optimal merge}, {Shannon-Fano coding}

# bib refs, eg, to defining article (pure HTML).  printed within <p>..</p>
@BIB=
<strong>David A. Huffman</strong>, <em>A Method for The Construction
of Minimum Redundancy Codes</em>, 
Proceedings of IRE, 40(9):1098-1101, September 1952.

# any notes.  these will not be printed in the final dictionary
@NOTES=
The {worst case} for Huffman coding (or, equivalently, the longest
Huffman coding for a set of characters) is when the distribution
of frequencies follows the {Fibonacci numbers}.  For this and other
relations see Alex Vinokur's note on
<a href="http://mathforum.org/kb/thread.jspa?messageID=3400647">Fibonacci
numbers, Lucas numbers and Huffman codes</a>.
</p>

<p>
Joining trees by frequency is the same as merging sequences by length
in {optimal merge}.  See the example there.
Since a node with only one child is not optimal, any Huffman
coding is a {full binary tree}.
</p>

<p>
Huffman coding is one of many <a
href="http://en.wikipedia.org/wiki/Category:Lossless_compression_algorithms">lossless
compression algorithms</a>.
</p>

<p>
Shannon-Fano is a minimal prefix code. Huffman is optimal for
character coding (one character-one code word) and simple to
program.  Arithmetic coding is a little better still, since it can
allocate fractional bits, but is more complicated and has patents.
# further explanation (pure HTML)
@LINKS=
A
<a
href="http://www.ics.uci.edu/~dan/pubs/DC-Sec3.html">survey on data
compression</a>,
John Morris'
# morris@ee.uwa.edu.au
<a
href="http://ciips.ee.uwa.edu.au/~morris/Year2/PLDS210/huffman.html">explanation,
example, and analysis</a>.
Wikipedia's encyclopedic article on
<a href="http://en.wikipedia.org/wiki/Huffman_coding">Huffman
coding</a>.
# implementation(s) (pure HTML)
@IMPL=
<a
href="http://www.cs.mu.oz.au/~alistair/abstracts/mk95:wads.html">build
tree in array (C)</a> --  also computes entropy and efficiency.
# author's initials
@AUTHOR=PEB
# end $Source: /home/black/Workspace/dads/Terms/RCS/huffmanCoding.trm,v $
