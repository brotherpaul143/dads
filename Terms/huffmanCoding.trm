# *created  "Wed Feb  3 13:42:31 1999" *by "Paul E. Black"
# *modified "Mon Mar 17 14:52:22 2003" *by "Paul E. Black"

# entry name
@NAME=Huffman encoding
# _A_lgorithm, _D_efinition, _P_roblem, or data _S_tructure
@TYPE=A
# basic, numeric, search, sort, graph, combin(atorial), tree, theory
@AREA=crypt
# the definition
@DEFN=A minimal variable-length character encoding based on the frequency of 
each character.  First, each character becomes a trivial {tree}, with the
character as the only {node}.  The character's frequency is the tree's
frequency.  The two trees with the least frequencies are joined with a
new root which is assigned the sum of their frequencies.  This is
repeated until all characters are in one tree.  One code bit represents
each level.  Thus more frequent characters are near the {root} and
are encoded with few bits, and rare characters are far from the root
and are encoded with many bits.
# formal definition or {cross reference} to an entry
@FORML=
# comma-sep list of pure names or {cross refs} that this is Also Known As.
@AKA=static Huffman encoding
# other cross-listings solely for the web, such as word or spelling variants
@WEB=Huffman coding, Hufman encoding, Huffmann coding
# comma-separated list of {cross references}, i.e., See also ...
@XREFS={arithmetic coding}, {k-ary Huffman encoding}, 
{adaptive Huffman encoding}, 
{optimal merge}, {greedy algorithm}, {full binary tree}, {Shannon-Fano coding}
# any notes.  these will not be printed in the final dictionary
@NOTES=
The {worst case} for Huffman encoding (or, equivalently, the longest
Huffman encoding for a set of characters) is when the distribution
of frequencies follows the {Fibonacci numbers}.  For this and other
relations see Alex Vinokur's note on
<A href="http://mathforum.org/discuss/sci.math/t/207334 ">Huffman
codes and Fibonacci numbers</A>.
</P>

<P>
Joining trees by frequency is the same as merging sequences by length
in {optimal merge}.  See the example there.
Since a node with only one child is not optimal, any Huffman
encoding is a {full binary tree}.
</P>

<P>
Shannon-Fano is a minimal prefix code. Huffman is optimal for
character encoding (one character-one code word) and simple to
program.  Arithmetic coding is better still, since it can allocate
fractional bits, but more complicated.
# further explanation (pure HTML)
@LINKS=
Dr. Pam J. Vermeer's <A
href="http://home.wlu.edu/~vermeerp/Classes/211w99/Lectures/Ch5_2/tsld001.htm">step
by step discovery of the algorithm</A>, 
<A
href="http://www.ics.uci.edu/~dan/pubs/DC-Sec3.html">survey on data
compression</A>,
# John Morris morris@ee.uwa.edu.au
<A
href="http://ciips.ee.uwa.edu.au/~morris/Year2/PLDS210/huffman.html">explanation,
example, and analysis</A>.
# implementation(s) (pure HTML)
@IMPL=
<A
href="http://www.cs.mu.oz.au/~alistair/abstracts/mk95:wads.html">build
tree in array (C)</A> --  also computes entropy and efficiency.
# author's initials
@AUTHOR=PEB
# end
